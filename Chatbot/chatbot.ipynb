{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bHlRFi47BAyg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHlRFi47BAyg",
    "outputId": "1d1b5442-6728-4e9f-f0d1-16d8f533eed1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eqy4aroMHDxI",
   "metadata": {
    "id": "eqy4aroMHDxI"
   },
   "outputs": [],
   "source": [
    "# Provide the path of the document to be used\n",
    "document_path = '/Users/nikhil/Desktop/Machine Learning/Natural Language Processing/Untitled.notepad'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcda72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01fcda72",
    "outputId": "fbd88059-469b-47db-8c69-e1bac9a177f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am the bot. Start typing your text.\n",
      "data science\n",
      "Bot: Its key components include data collection, data cleaning and preprocessing, exploratory data analysis, statistical modeling, machine learning, and data visualization.\\nExplain the data science workflow or lifecycle.\\n\\nThe data science workflow typically involves several stages: data acquisition, data cleaning and preprocessing, exploratory data analysis, feature engineering, model building, model evaluation, and deployment.\n",
      "\n",
      "bagging\n",
      "Bot: Bagging involves training multiple models on different subsets of the data and averaging their predictions, aiming to reduce variance.\n",
      "\n",
      "crossvalidation\n",
      "Bot: Dimensionality reduction, such as principal component analysis (PCA), aims to reduce the number of features while preserving the most important information in the data.\\nWhat is the purpose of cross-validation in machine learning?\\n\\nCross-validation is a technique used to assess the performance of a machine learning model.\n",
      "\n",
      "How do you handle missing values in a datase\n",
      "Bot: Cross-validation helps estimate how well the model will perform on unseen data.\\nHow do you handle missing values in a dataset?\\n\\nMissing values can be handled by either removing the rows or columns with missing data, filling them with a specific value (e.g., mean, median, or mode), or using more advanced techniques like interpolation or predictive models to estimate missing values.\\nWhat is the difference between bagging and boosting techniques?\\n\\nBagging and boosting are ensemble learning techniques.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "greet_inputs = ('hello', 'hi', 'how are you')\n",
    "greet_responses = ('hi', 'hey', 'hey there')\n",
    "\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)\n",
    "    return None\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n",
    "\n",
    "def response(user_response):\n",
    "    robol_response = ''\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens + [user_response])\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if req_tfidf == 0:\n",
    "        robol_response = \"I am unable to understand you.\"\n",
    "    else:\n",
    "        robol_response = sentence_tokens[idx]\n",
    "    \n",
    "    return robol_response\n",
    "\n",
    "# Provide the path of the document to be used\n",
    "#document_path = '/content/drive/MyDrive/Resume/Untitled.notepad'\n",
    "\n",
    "flag = True\n",
    "sentence_tokens = []\n",
    "print(\"Hello, I am the bot. Start typing your text.\")\n",
    "\n",
    "# Read the document and tokenize the sentences\n",
    "with open(document_path, 'r', errors='ignore') as file:\n",
    "    document_content = file.read()\n",
    "    sentence_tokens = nltk.sent_tokenize(document_content)\n",
    "\n",
    "while flag:\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "\n",
    "    if user_response != 'bye':\n",
    "        if user_response == 'thank you' or user_response == 'thanks':\n",
    "            flag = False\n",
    "            print('Bot: You are welcome')\n",
    "        else:\n",
    "            if greet(user_response) is not None:\n",
    "                print('Bot:', greet(user_response))\n",
    "            else:\n",
    "                print('Bot:', response(user_response))\n",
    "                print()\n",
    "    else:\n",
    "        flag = False\n",
    "        print('Bot: Goodbye')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xp33C3YVHEiB",
   "metadata": {
    "id": "Xp33C3YVHEiB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
